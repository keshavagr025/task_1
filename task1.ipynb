{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0822f5-c628-4b83-8883-716c52bb524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import heapq\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Input: Long article text\n",
    "article_text = \"\"\"\n",
    "Artificial Intelligence (AI) is transforming the way we interact with technology. It is being integrated \n",
    "into everything from our phones and home assistants to our vehicles and healthcare systems. \n",
    "AI technologies such as natural language processing, computer vision, and machine learning algorithms \n",
    "are enabling machines to understand, interpret, and even generate human language and visuals. \n",
    "The rise of generative models like GPT-4 and diffusion models in art have also opened new doors \n",
    "in creativity and productivity. While AI holds immense promise, it also raises ethical concerns \n",
    "regarding privacy, job displacement, and bias in algorithms. Governments and organizations are \n",
    "actively exploring regulations and guidelines to ensure AI is developed and deployed responsibly.\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: Tokenize into sentences\n",
    "sentences = sent_tokenize(article_text)\n",
    "\n",
    "# Step 2: Tokenize into words and calculate word frequencies\n",
    "stop_words = set(stopwords.words('english') + list(punctuation))\n",
    "word_frequencies = {}\n",
    "\n",
    "for word in word_tokenize(article_text.lower()):\n",
    "    if word not in stop_words:\n",
    "        if word not in word_frequencies:\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1\n",
    "\n",
    "# Step 3: Normalize word frequencies\n",
    "max_freq = max(word_frequencies.values())\n",
    "for word in word_frequencies:\n",
    "    word_frequencies[word] = word_frequencies[word] / max_freq\n",
    "\n",
    "# Step 4: Score each sentence based on word frequencies\n",
    "sentence_scores = {}\n",
    "for sent in sentences:\n",
    "    for word in word_tokenize(sent.lower()):\n",
    "        if word in word_frequencies:\n",
    "            if len(sent.split(' ')) < 40:  # optional: skip long sentences\n",
    "                if sent not in sentence_scores:\n",
    "                    sentence_scores[sent] = word_frequencies[word]\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_frequencies[word]\n",
    "\n",
    "# Step 5: Select top sentences (3 in this case)\n",
    "summary_sentences = heapq.nlargest(3, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "# Step 6: Combine sentences for final summary\n",
    "summary = ' '.join(summary_sentences)\n",
    "\n",
    "# ✅ Output\n",
    "print(\"📄 Original Article:\\n\", article_text)\n",
    "print(\"\\n📝 Extractive Summary:\\n\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53971a7-e21d-435d-9bee-819f07af6a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import heapq\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Input: Long article text\n",
    "article_text = \"\"\"\n",
    "Artificial Intelligence (AI) is transforming the way we interact with technology. It is being integrated \n",
    "into everything from our phones and home assistants to our vehicles and healthcare systems. \n",
    "AI technologies such as natural language processing, computer vision, and machine learning algorithms \n",
    "are enabling machines to understand, interpret, and even generate human language and visuals. \n",
    "The rise of generative models like GPT-4 and diffusion models in art have also opened new doors \n",
    "in creativity and productivity. While AI holds immense promise, it also raises ethical concerns \n",
    "regarding privacy, job displacement, and bias in algorithms. Governments and organizations are \n",
    "actively exploring regulations and guidelines to ensure AI is developed and deployed responsibly.\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: Tokenize into sentences\n",
    "sentences = sent_tokenize(article_text)\n",
    "\n",
    "# Step 2: Tokenize into words and calculate word frequencies\n",
    "stop_words = set(stopwords.words('english') + list(punctuation))\n",
    "word_frequencies = {}\n",
    "\n",
    "for word in word_tokenize(article_text.lower()):\n",
    "    if word not in stop_words:\n",
    "        if word not in word_frequencies:\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1\n",
    "\n",
    "# Step 3: Normalize word frequencies\n",
    "max_freq = max(word_frequencies.values())\n",
    "for word in word_frequencies:\n",
    "    word_frequencies[word] = word_frequencies[word] / max_freq\n",
    "\n",
    "# Step 4: Score each sentence based on word frequencies\n",
    "sentence_scores = {}\n",
    "for sent in sentences:\n",
    "    for word in word_tokenize(sent.lower()):\n",
    "        if word in word_frequencies:\n",
    "            if len(sent.split(' ')) < 40:  # optional: skip long sentences\n",
    "                if sent not in sentence_scores:\n",
    "                    sentence_scores[sent] = word_frequencies[word]\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_frequencies[word]\n",
    "\n",
    "# Step 5: Select top sentences (3 in this case)\n",
    "summary_sentences = heapq.nlargest(3, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "# Step 6: Combine sentences for final summary\n",
    "summary = ' '.join(summary_sentences)\n",
    "\n",
    "# ✅ Output\n",
    "print(\"📄 Original Article:\\n\", article_text)\n",
    "print(\"\\n📝 Extractive Summary:\\n\", summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
